openapi: 3.0.3
info:
  title: Ollama Generate API
  description: OpenAPI specification for Ollama's text generation endpoint
  version: 1.0.0
servers:
  - url: http://localhost:11434
    description: Local Ollama server

paths:
  /api/generate:
    post:
      summary: Generate text completion
      description: Generate text based on a prompt using the specified model
      operationId: generateCompletion
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required:
                - model
                - prompt
              properties:
                model:
                  type: string
                  description: Name of the model to use
                  example: "llama3.2:3b"
                prompt:
                  type: string
                  description: The input prompt for text generation
                  example: "Why is the sky blue?"
                images:
                  type: array
                  items:
                    type: string
                  description: Base64-encoded images (for multimodal models)
                format:
                  type: string
                  description: Format to return (json)
                  default: json
                  enum:
                    - json
                options:
                  type: object
                  description: Advanced generation parameters
                  properties:
                    temperature:
                      type: number
                      description: Sampling temperature (0.0 to 1.0)
                      default: 0.8
                      minimum: 0.0
                      maximum: 1.0
                    top_p:
                      type: number
                      description: Nucleus sampling parameter
                      default: 0.9
                    top_k:
                      type: integer
                      description: Top-k sampling parameter
                      default: 40
                    num_predict:
                      type: integer
                      description: Maximum number of tokens to predict
                      default: 128
                    stop:
                      type: array
                      items:
                        type: string
                      description: Stop sequences
                    seed:
                      type: integer
                      description: Random seed for reproducibility
                stream:
                  type: boolean
                  description: Stream response as JSON objects
                  default: true
                context:
                  type: array
                  items:
                    type: integer
                  description: Context from previous request for conversation continuity
                raw:
                  type: boolean
                  description: Disable formatting (for models with prompt templates)
                  default: false
                keep_alive:
                  type: string
                  description: How long to keep model loaded in memory
                  default: "5m"
                  example: "30m"
            examples:
              simple:
                summary: Simple text generation
                value:
                  model: "llama3.2:3b"
                  prompt: "Write a haiku about programming"
                  stream: false
              with_options:
                summary: Generation with custom options
                value:
                  model: "llama3.2:3b"
                  prompt: "Explain quantum computing"
                  stream: false
                  options:
                    temperature: 0.7
                    num_predict: 200
      responses:
        '200':
          description: Successful text generation
          content:
            application/json:
              schema:
                oneOf:
                  - $ref: '#/components/schemas/GenerateResponse'
                  - $ref: '#/components/schemas/StreamResponse'
        '400':
          description: Bad request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
        '404':
          description: Model not found
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'

components:
  schemas:
    GenerateResponse:
      type: object
      description: Non-streaming response
      properties:
        model:
          type: string
          description: Model used for generation
        created_at:
          type: string
          format: date-time
          description: Timestamp of generation
        response:
          type: string
          description: Generated text
        done:
          type: boolean
          description: Whether generation is complete
        context:
          type: array
          items:
            type: integer
          description: Context for continuing conversation
        total_duration:
          type: integer
          description: Total generation time in nanoseconds
        load_duration:
          type: integer
          description: Model load time in nanoseconds
        prompt_eval_count:
          type: integer
          description: Number of tokens in prompt
        prompt_eval_duration:
          type: integer
          description: Prompt evaluation time in nanoseconds
        eval_count:
          type: integer
          description: Number of tokens generated
        eval_duration:
          type: integer
          description: Generation time in nanoseconds
      example:
        model: "llama3.2:3b"
        created_at: "2024-01-01T12:00:00Z"
        response: "The sky appears blue due to Rayleigh scattering..."
        done: true
        total_duration: 2500000000
        eval_count: 45

    StreamResponse:
      type: object
      description: Streaming response (when stream=true)
      properties:
        model:
          type: string
        created_at:
          type: string
          format: date-time
        response:
          type: string
          description: Partial generated text
        done:
          type: boolean
          description: Whether this is the final chunk

    ErrorResponse:
      type: object
      properties:
        error:
          type: string
          description: Error message
      example:
        error: "model 'unknown:latest' not found"